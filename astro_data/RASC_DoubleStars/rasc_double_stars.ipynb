{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5541b9b-eb35-43ac-a087-5ffb2922aee0",
   "metadata": {},
   "source": [
    "# Preparing the RASC Double Stars catalog for import into PiFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8af58-60ab-446b-8411-c63710afb292",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115ba43-d4dc-4d9d-b559-f99113cefbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a68c88-a944-40f5-a86a-6787c52310e6",
   "metadata": {},
   "source": [
    "## importing the supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4a5a6-3e11-4199-8dc6-47c1851114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'./RASC DS supplement 210710-import.xlsx')\n",
    "df.columns = df.columns.str.strip()\n",
    "df.drop(['Unnamed: 10', 'Unnamed: 11'], axis=1, inplace=True)\n",
    "# drop repeated headers\n",
    "rows_to_drop = df[df.apply(lambda x: (x == df.columns).all(), axis=1)].index\n",
    "# Drop the identified rows\n",
    "df.drop(index=rows_to_drop, inplace=True)\n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777e29c-5ab8-4703-8521-41c22d9b44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'] = None\n",
    "\n",
    "# Variable to hold the current season value\n",
    "current_season = None\n",
    "seasons = {\n",
    "    \"WINTER (part 1)\": \"winter(1)\",\n",
    "    \"WINTER (part 2)\": \"winter(2)\",\n",
    "    \"SPRING (part 1)\": \"spring(1)\",\n",
    "    \"SPRING (part 2)\": \"spring(2)\", \n",
    "    \"SPRING (part 3)\": \"spring(3)\",\n",
    "    \"SUMMER (part 1)\": \"summer(1)\",\n",
    "    \"SUMMER (part 2)\": \"summer(2)\",\n",
    "    \"SUMMER (part 3)\": \"summer(2)\",\n",
    "    \"AUTUMN (part 1)\": \"autumn(1)\",\n",
    "    \"AUTUMN (part 2)\": \"autumn(2)\",\n",
    "}\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the 'Type' column is not NaN\n",
    "    if pd.isna(row['Type']) and not pd.isna(row['Target']):\n",
    "        # Update the current season using the value in the 'Target' column\n",
    "        current_season = row['Target']\n",
    "        #print(f\"Updating to {current_season=} because of {row=}\")\n",
    "    else:\n",
    "        # Set the current season for the row\n",
    "        #print(current_season)\n",
    "        df.at[index, 'season'] = seasons[current_season] if current_season is not None else nan\n",
    "# Remove rows where 'Type' is NaN\n",
    "df = df.dropna(subset=['Type'])\n",
    "df['Pair'].fillna('AB', inplace=True)\n",
    "df = df.loc[df['Target'] != 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bab78-4d0b-4dfa-af0a-7c2a334897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_df = df\n",
    "supplement_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4001d-7990-48f1-bbae-b7326b5f445e",
   "metadata": {},
   "source": [
    "## importing the main catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af403290-bed9-4521-b1fc-cc5318aa04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'./RASC DS main checklist 210710.xlsx', skiprows=16)\n",
    "df.columns = df.columns.str.strip()\n",
    "# fix mismatch in name\n",
    "df['Target'] = df['Target'].replace('The Trapezium', 'Trapezium')\n",
    "df['Target'] = df['Target'].replace('U  Cyg', 'U Cyg')\n",
    "df.drop(['Seen?'], axis=1, inplace=True)\n",
    "# drop repeated headers\n",
    "rows_to_drop = df[df.apply(lambda x: (x == df.columns).all(), axis=1)].index\n",
    "# Drop the identified rows\n",
    "df.drop(index=rows_to_drop, inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca664f0-6dc7-4b4a-9a2c-d6ebf4c8a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d0957-fd10-400c-839a-fc1884a61388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Type' is NaN\n",
    "df = df.dropna(subset=['PSA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5dba0-a912-4382-8ffa-a01efb9cbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = df\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53588b6f-4413-4556-af81-942761bacb06",
   "metadata": {},
   "source": [
    "## Consolidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c84562-7dee-43a4-9b03-940b915f96c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(main_df.columns)\n",
    "print(supplement_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964cec6-521c-4431-b491-a728a2f8fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an outer join with supplement_df as the left DataFrame\n",
    "merged_df = pd.merge(supplement_df, main_df, on='Target', how='left', suffixes=('_supp', ''))\n",
    "\n",
    "# Strip all strings\n",
    "merged_df = merged_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Print the columns of the merged DataFrame to verify the merge\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380e430-3dc3-4e5e-9614-15918b222495",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(['Con_supp', 'WDS'], axis=1, inplace=True)\n",
    "# Keep rows where 'Target' column values are not 'Target'\n",
    "merged_df = merged_df.loc[merged_df['Target'] != 'Target']\n",
    "merged_df.rename(columns={\n",
    "    'WDS_supp': 'WDS',\n",
    "    'Sep': 'SepSec'\n",
    "}, inplace=True)\n",
    "merged_df = merged_df[['Target', 'Alternate ID', 'SAO', 'HIP', 'WDS', 'Con', 'RA 2000',\n",
    "       'Dec 2000', 'Mm', 'X', 'PSA', 'season', 'Parent', 'Pair',\n",
    "       'PA', 'SepSec', 'MagC', 'M1', 'M2', 'Notes', 'Type']]\n",
    "merged_df = merged_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ab6ad-541b-462f-a654-c9522ba4f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839aec45-b819-4e55-871a-0fe62b19d909",
   "metadata": {},
   "source": [
    "## format for PiFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab178b-c272-4021-ab01-536064bb1a3f",
   "metadata": {},
   "source": [
    "### first an export of the full dataset, before pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57555e35-1bc0-4c5d-ba5c-835a95e5d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('rasc_double_stars_full.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d92d7-d7f4-436e-827a-5d460f6ba499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.drop(['SAO', 'HIP', 'PSA', 'Parent'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf59ca-aeb5-4e9e-842a-a739827f80c3",
   "metadata": {},
   "source": [
    "### Group clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238139ec-0a88-4bfe-ac1d-5b72f5edd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_target = df['Target'] != ''\n",
    "\n",
    "# Use cumsum on the boolean series to create group identifiers\n",
    "df['group'] = non_empty_target.cumsum()\n",
    "# Group the DataFrame by the 'group' column\n",
    "grouped = df.groupby('group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce26a05-a4a8-4d94-a8e1-0a01efd76220",
   "metadata": {},
   "source": [
    "### Iterate over groups and extract consolidated notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f28dc3-2c10-4dd5-ad97-cc45ef0729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "out_df = pd.DataFrame(columns=['Target', 'AlternateID', 'WDS', 'Con', 'RA2000', 'Dec2000', 'Mag', 'MaxSepSec', 'Notes'])\n",
    "\n",
    "def extract_info(row):\n",
    "    result = f\"\"\"ï€…{row['Pair']} ({row['Type']}), PA: {row['PA']}, Sep: {row['SepSec']}<SECS>, Mag: {row['M1']}/{row['M2']} {row['Notes']}\"\"\"\n",
    "    return result.replace('\"', '<SECS>')\n",
    "    \n",
    "# Iterate over each group\n",
    "for group_number, group_df in grouped:\n",
    "    #print(f\"Processing group {group_number}, {len(group_df)}\")\n",
    "    # 'group_df' is a DataFrame containing only the rows from the current group\n",
    "\n",
    "    new_row = {'Notes': ''}\n",
    "    max_sep = -1\n",
    "    # Perform your operations on each group member here\n",
    "    # For example, let's print the first row of each group\n",
    "    letterset = set()\n",
    "    for index, row in group_df.iterrows():\n",
    "        pairs = row['Pair'].split(',')\n",
    "        for pair in pairs:\n",
    "            letters = [*pair]\n",
    "            if letters[1].islower():\n",
    "                letterset.discard(letters[0])\n",
    "                letters = [letters[1]]\n",
    "            letterset.update(letters)\n",
    "    #print(f\"{group_number=}, {letterset=}\")\n",
    "    if len(letterset) != len(group_df)+1:\n",
    "        print(f\"Lengnth of letterset is {len(letterset)} == {len(group_df)+1}, {letterset=}, {group_number=}, {group_df=}\")\n",
    "    nr_stars = len(letterset)\n",
    "         \n",
    "    for index, row in group_df.iterrows():\n",
    "        if row['SepSec'] > max_sep:\n",
    "            max_sep = row['SepSec']\n",
    "        if row['Target']:\n",
    "            new_row = {\n",
    "            'Target': row['Target'],\n",
    "            'AlternateID': row['Alternate ID'],\n",
    "            'WDS': row['WDS'],\n",
    "            'Con': row['Con'],\n",
    "            'RA2000': row['RA 2000'], \n",
    "            'Dec2000': row['Dec 2000'],\n",
    "            'Mag': [row['MagC'], row['M1'], row['M2']],\n",
    "            'Notes': f\"{nr_stars}ï€…,ó°¥®{row['Mm']/10:g}cm,{row['X']:g}x,{row['season']}<NEWLINE>{extract_info(row)}\"\n",
    "            }\n",
    "        else:\n",
    "            new_row['Notes'] = new_row['Notes'] + '<NEWLINE>' + extract_info(row)\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    new_row['MaxSepSec'] = max_sep\n",
    "    out_df = pd.concat([out_df,  pd.DataFrame([new_row])], ignore_index=True)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1517177-3ff7-48d9-af59-7c88f2137926",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.index = out_df.index + 1\n",
    "out_df = out_df.reset_index().rename(columns={'index': 'sequence'})\n",
    "out_df.to_csv('rasc_double_stars.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd4803-685b-4db4-81f1-8a1e4356c35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
